{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIcF9GDx3PMt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import timedelta\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prophet and Hyperopt imports\n",
        "import cmdstanpy\n",
        "cmdstanpy.set_cmdstan_path('/root/.cmdstan/cmdstan-2.37.0') # Ensure CmdStan path is set before Prophet import\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "except Exception as e:\n",
        "    raise ImportError(\"Prophet (fbprophet/prophet) must be installed. pip install prophet\") from e\n",
        "\n",
        "try:\n",
        "    from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "except Exception as e:\n",
        "    raise ImportError(\"hyperopt must be installed. pip install hyperopt\") from e\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mMyKO9k-7lUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Config / Paths\n",
        "# -----------------------------\n",
        "DATA_PATH = '/content/stock_data.csv'   # <- Ensure 'stock_data.csv' is uploaded and accessible here\n",
        "OUTPUT_DIR = '/mnt/data/prophet_bayes_opt_outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "Ktv5NY8Z7oIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Utility metrics\n",
        "# -----------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def wape(y_true, y_pred):\n",
        "    # Weighted Absolute Percentage Error (sum absolute errors / sum actuals)\n",
        "    denom = np.sum(np.abs(y_true))\n",
        "    if denom == 0:\n",
        "        return np.nan\n",
        "    return np.sum(np.abs(y_true - y_pred)) / denom"
      ],
      "metadata": {
        "id": "tgvvo2oO7wOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1. LOAD & PREPROCESS\n",
        "# -----------------------------\n",
        "def load_and_preprocess(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # Normalize column names commonly used: Date, Close or date, close\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    # find date-like and value-like columns\n",
        "    date_col = None\n",
        "    value_col = None\n",
        "    for c in df.columns:\n",
        "        if c.lower() in ('date', 'ds', 'timestamp', 'time', 'unnamed: 0'): # Added 'unnamed: 0'\n",
        "            date_col = c\n",
        "        if c.lower() in ('close', 'y', 'price', 'value', 'stock_1'): # Added 'stock_1'\n",
        "            value_col = c\n",
        "    if date_col is None:\n",
        "        raise ValueError('No date column found. Expected column named Date/ds/timestamp/Unnamed: 0. Columns: %s' % (df.columns.tolist(),))\n",
        "    if value_col is None:\n",
        "        raise ValueError('No value column found. Expected Close/y/price/value/Stock_1. Columns: %s' % (df.columns.tolist(),))\n",
        "    df = df[[date_col, value_col]].copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "    df = df.rename(columns={date_col: 'ds', value_col: 'y'})\n",
        "    # Drop missing values\n",
        "    df = df.dropna(subset=['ds','y']).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "EMzixDTX7wid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2. Rolling-origin cross-validation (custom wrapper around TimeSeriesSplit)\n",
        "# -----------------------------\n",
        "def rolling_origin_splits(df, n_splits=5, test_size=None):\n",
        "    # If test_size is None, use length//(n_splits+1) as holdout window\n",
        "    N = len(df)\n",
        "    if test_size is None:\n",
        "        test_size = max(1, N // (n_splits + 1))\n",
        "    splits = []\n",
        "    for i in range(n_splits):\n",
        "        train_end = test_size * (i + 1)\n",
        "        train_df = df.iloc[:train_end].copy()\n",
        "        val_start = train_end\n",
        "        val_end = val_start + test_size\n",
        "        val_df = df.iloc[val_start:val_end].copy()\n",
        "        if len(train_df) < 2 or len(val_df) < 1:\n",
        "            continue\n",
        "        splits.append((train_df, val_df))\n",
        "    return splits\n"
      ],
      "metadata": {
        "id": "Qb2rDjXR78tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3. Objective function for Hyperopt\n",
        "# -----------------------------\n",
        "def prophet_cv_score(params, df, n_splits=4, test_size=None, verbose=False):\n",
        "    \"\"\"Compute average RMSE across rolling-origin splits for given hyperparameters.\n",
        "    Params is expected to be a dict with numeric values for Prophet hyperparameters.\n",
        "    \"\"\"\n",
        "    splits = rolling_origin_splits(df, n_splits=n_splits, test_size=test_size)\n",
        "    rmses = []\n",
        "    maes = []\n",
        "    wapes = []\n",
        "    for idx, (train_df, val_df) in enumerate(splits):\n",
        "        model = Prophet(\n",
        "            changepoint_prior_scale=float(params['changepoint_prior_scale']),\n",
        "            seasonality_prior_scale=float(params['seasonality_prior_scale']),\n",
        "            holidays_prior_scale=float(params.get('holidays_prior_scale', 0.0)),\n",
        "            seasonality_mode=params.get('seasonality_mode', 'additive'),\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=True,\n",
        "            daily_seasonality=False\n",
        "        )\n",
        "        # Fit & predict\n",
        "        model.fit(train_df)\n",
        "        future = model.make_future_dataframe(periods=len(val_df), freq='D')\n",
        "        forecast = model.predict(future)\n",
        "        # Align predictions to validation ds\n",
        "        pred = forecast.set_index('ds').loc[val_df['ds']]\n",
        "        y_true = val_df['y'].values\n",
        "        y_pred = pred['yhat'].values\n",
        "        r = rmse(y_true, y_pred)\n",
        "        m = mean_absolute_error(y_true, y_pred)\n",
        "        w = wape(y_true, y_pred)\n",
        "        rmses.append(r); maes.append(m); wapes.append(w)\n",
        "        if verbose:\n",
        "            print(f\"Fold {idx+1}: RMSE={r:.4f}, MAE={m:.4f}, WAPE={w:.4f}\")\n",
        "    return {\n",
        "        'rmse_mean': float(np.mean(rmses)),\n",
        "        'rmse_fold': [float(x) for x in rmses],\n",
        "        'mae_mean': float(np.mean(maes)),\n",
        "        'wape_mean': float(np.nanmean(wapes))\n",
        "    }"
      ],
      "metadata": {
        "id": "dNaeZ3928CNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4. Hyperparameter search space (Hyperopt)\n",
        "# -----------------------------\n",
        "def run_hyperopt(df, max_evals=40, n_splits=4, test_size=None, random_state=42):\n",
        "    space = {\n",
        "        'changepoint_prior_scale': hp.uniform('changepoint_prior_scale', 0.001, 0.5),\n",
        "        'seasonality_prior_scale': hp.uniform('seasonality_prior_scale', 0.1, 30.0),\n",
        "        'holidays_prior_scale': hp.uniform('holidays_prior_scale', 0.0, 20.0),\n",
        "        'seasonality_mode': hp.choice('seasonality_mode', ['additive', 'multiplicative']),\n",
        "    }\n",
        "    trials = Trials()\n",
        "\n",
        "    def objective(hparams):\n",
        "        # Ensure deterministic sampling order doesn't break numeric formatting\n",
        "        hparams['seasonality_mode'] = ['additive','multiplicative'][int(hparams['seasonality_mode'])] if isinstance(hparams.get('seasonality_mode'), (int, np.integer)) else hparams.get('seasonality_mode')\n",
        "        score = prophet_cv_score(hparams, df, n_splits=n_splits, test_size=test_size, verbose=False)\n",
        "        loss = float(score['rmse_mean'])\n",
        "        return {'loss': loss, 'status': STATUS_OK, 'attachments': {'score': score}, 'params': hparams}\n",
        "\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials, rstate=np.random.RandomState(random_state))\n",
        "    # Resolve the seasonality_mode index if necessary (Hyperopt sometimes returns index)\n",
        "    if isinstance(best.get('seasonality_mode'), int):\n",
        "        best['seasonality_mode'] = ['additive','multiplicative'][best['seasonality_mode']]\n",
        "    return best, trials"
      ],
      "metadata": {
        "id": "EF1OQ2SO8Ch7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5. Train final models and evaluate on a held-out test set\n",
        "# -----------------------------\n",
        "def train_and_evaluate(df, best_params, test_days=90):\n",
        "    # Split last `test_days` as test set\n",
        "    cutoff = df['ds'].max() - pd.Timedelta(days=test_days)\n",
        "    train_df = df[df['ds'] <= cutoff].copy()\n",
        "    test_df = df[df['ds'] > cutoff].copy()\n",
        "    if len(test_df) < 1:\n",
        "        raise ValueError('Not enough points for a test set. Increase dataset or reduce test_days.')\n",
        "\n",
        "    # Final optimized model\n",
        "    model_opt = Prophet(\n",
        "        changepoint_prior_scale=float(best_params['changepoint_prior_scale']),\n",
        "        seasonality_prior_scale=float(best_params['seasonality_prior_scale']),\n",
        "        holidays_prior_scale=float(best_params.get('holidays_prior_scale', 0.0)),\n",
        "        seasonality_mode=best_params.get('seasonality_mode', 'additive'),\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=True\n",
        "    )\n",
        "    model_opt.fit(train_df)\n",
        "    future_opt = model_opt.make_future_dataframe(periods=len(test_df), freq='D')\n",
        "    forecast_opt = model_opt.predict(future_opt).set_index('ds').loc[test_df['ds']].reset_index()\n",
        "\n",
        "    # Baseline (default Prophet)\n",
        "    model_base = Prophet()\n",
        "    model_base.fit(train_df)\n",
        "    future_base = model_base.make_future_dataframe(periods=len(test_df), freq='D')\n",
        "    forecast_base = model_base.predict(future_base).set_index('ds').loc[test_df['ds']].reset_index()\n",
        "\n",
        "    # Metrics\n",
        "    y_true = test_df['y'].values\n",
        "    y_opt = forecast_opt['yhat'].values\n",
        "    y_base = forecast_base['yhat'].values\n",
        "\n",
        "    results = {\n",
        "        'optimized': {\n",
        "            'rmse': rmse(y_true, y_opt),\n",
        "            'mae': mean_absolute_error(y_true, y_opt),\n",
        "            'wape': wape(y_true, y_opt)\n",
        "        },\n",
        "        'baseline': {\n",
        "            'rmse': rmse(y_true, y_base),\n",
        "            'mae': mean_absolute_error(y_true, y_base),\n",
        "            'wape': wape(y_true, y_base)\n",
        "        },\n",
        "        'test_size': len(test_df)\n",
        "    }\n",
        "\n",
        "    return model_opt, model_base, forecast_opt, forecast_base, results, train_df, test_df"
      ],
      "metadata": {
        "id": "QrcR-yyV8LxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6. Plotting utilities\n",
        "# -----------------------------\n",
        "def plot_forecasts(train_df, test_df, forecast_opt, forecast_base, outpath):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(train_df['ds'], train_df['y'], label='Train (y)', linewidth=1)\n",
        "    plt.plot(test_df['ds'], test_df['y'], label='Test (y)', linewidth=1)\n",
        "    plt.plot(forecast_opt['ds'], forecast_opt['yhat'], label='Optimized Forecast', linestyle='--')\n",
        "    plt.plot(forecast_base['ds'], forecast_base['yhat'], label='Baseline Forecast', linestyle=':')\n",
        "    plt.legend()\n",
        "    plt.xlabel('ds'); plt.ylabel('y')\n",
        "    plt.title('Train/Test and Forecast Comparison')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath)\n",
        "    plt.close()\n",
        "\n",
        "def plot_convergence(trials, outpath, top_n=40):\n",
        "    # Extract losses in order of evals\n",
        "    losses = [t['result']['loss'] for t in trials.trials if 'result' in t and 'loss' in t['result']]\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
        "    plt.xlabel('Evaluation #')\n",
        "    plt.ylabel('Loss (RMSE)')\n",
        "    plt.title('Hyperopt Convergence (RMSE per eval)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "aOsjbfm-8Tsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 7. Save JSON/text report\n",
        "# -----------------------------\n",
        "def save_report(output_dir, best_params, trials, cv_summary, eval_results):\n",
        "    report = {\n",
        "        'description': 'Prophet Bayesian Hyperparameter Optimization project report',\n",
        "        'data_path': DATA_PATH,\n",
        "        'best_params': best_params,\n",
        "        'cv_summary': cv_summary,\n",
        "        'evaluation': eval_results\n",
        "    }\n",
        "    report_path = os.path.join(output_dir, 'report.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2, default=lambda o: str(o))\n",
        "    # Also save a human-readable text summary\n",
        "    text_path = os.path.join(output_dir, 'report.txt')\n",
        "    with open(text_path, 'w') as f:\n",
        "        f.write('Prophet Bayesian Hyperparameter Optimization Report\\n\\n')\n",
        "        f.write('Data path: %s\\n' % DATA_PATH)\n",
        "        f.write('\\nBest hyperparameters:\\n')\n",
        "        f.write(json.dumps(best_params, indent=2) + '\\n\\n')\n",
        "        f.write('Cross-validation summary:\\n')\n",
        "        f.write(json.dumps(cv_summary, indent=2) + '\\n\\n')\n",
        "        f.write('Evaluation (test set):\\n')\n",
        "        f.write(json.dumps(eval_results, indent=2) + '\\n')\n",
        "    return report_path, text_path\n"
      ],
      "metadata": {
        "id": "HO6odtbZ8XMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# MAIN\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print('Loading data from:', DATA_PATH)\n",
        "    df = load_and_preprocess(DATA_PATH)\n",
        "    print('Rows loaded:', len(df))\n",
        "    if len(df) < 365*3:\n",
        "        print('WARNING: dataset has fewer than 3 years of daily data. Project requirement is at least 3 years.')\n",
        "\n",
        "    # CV hyperparameters\n",
        "    n_cv_splits = 4\n",
        "    test_size = max(30, len(df) // (n_cv_splits + 2))  # conservative holdout window\n",
        "    print(f'Using rolling-origin CV with {n_cv_splits} splits, test_size={test_size} days per split.')\n",
        "\n",
        "    # Hyperopt search\n",
        "    print('Starting Hyperopt search... (this may take time depending on max_evals and dataset size)')\n",
        "    best, trials = run_hyperopt(df, max_evals=30, n_splits=n_cv_splits, test_size=test_size)\n",
        "    print('Raw best (Hyperopt encoding):', best)\n",
        "    # Convert numbers to floats\n",
        "    best_params = {k: (float(v) if isinstance(v, (int, np.integer, float, np.floating)) else v) for k,v in best.items()}\n",
        "\n",
        "    # Compute CV summary of best\n",
        "    print('Computing CV summary for best hyperparameters...')\n",
        "    cv_summary = prophet_cv_score(best_params, df, n_splits=n_cv_splits, test_size=test_size, verbose=True)\n",
        "\n",
        "    # Train final models and evaluate on a held out test set (last 90 days)\n",
        "    print('Training final models and evaluating on last 90 days...')\n",
        "    model_opt, model_base, forecast_opt, forecast_base, eval_results, train_df, test_df = train_and_evaluate(df, best_params, test_days=90)\n",
        "\n",
        "    # Save visualizations\n",
        "    fig1 = os.path.join(OUTPUT_DIR, 'forecast_comparison.png')\n",
        "    plot_forecasts(train_df, test_df, forecast_opt, forecast_base, fig1)\n",
        "    fig2 = os.path.join(OUTPUT_DIR, 'hyperopt_convergence.png')\n",
        "    plot_convergence(trials, fig2)\n",
        "\n",
        "    # Save models? Prophet models can be pickled but may be large / env-specific.\n",
        "    # We'll save the best_params and a report\n",
        "    report_json, report_txt = save_report(OUTPUT_DIR, best_params, trials, cv_summary, eval_results)\n",
        "    # Also save best params separately\n",
        "    with open(os.path.join(OUTPUT_DIR, 'best_params.json'), 'w') as f:\n",
        "        json.dump(best_params, f, indent=2)\n",
        "\n",
        "    # Print final summary\n",
        "    print('\\n=== FINAL SUMMARY ===')\n",
        "    print('Best hyperparameters:')\n",
        "    print(json.dumps(best_params, indent=2))\n",
        "    print('\\nCross-validation summary:')\n",
        "    print(json.dumps(cv_summary, indent=2))\n",
        "    print('\\nTest evaluation (last 90 days):')\n",
        "    print(json.dumps(eval_results, indent=2))\n",
        "    print('\\nArtifacts saved to:', OUTPUT_DIR)\n",
        "    print('- report.json, report.txt, best_params.json')\n",
        "    print('- forecast_comparison.png, hyperopt_convergence.png')\n",
        "    print('\\nTo inspect visualizations, open the PNG files in %s' % OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "LtdoB8L28bwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}